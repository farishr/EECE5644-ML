{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torchinfo import summary\n",
    "from captum.attr import Saliency\n",
    "import pickle\n",
    "torch.manual_seed(42)\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # For M1 chip, torch.device(\"mps\") is used instead of torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image_0': {'image': tensor([[[[-2.0323, -2.0323, -2.0323,  ..., -1.0390, -0.9705, -0.9192],\n",
       "            [-2.0323, -2.0323, -2.0323,  ..., -1.0219, -0.9534, -0.9020],\n",
       "            [-2.0323, -2.0323, -2.0323,  ..., -0.9705, -0.9534, -0.9534],\n",
       "            ...,\n",
       "            [ 1.9064,  1.8722,  1.9578,  ...,  1.2214,  1.1015,  0.9988],\n",
       "            [ 1.6667,  1.5810,  1.8037,  ...,  1.1700,  1.0331,  0.9817],\n",
       "            [ 1.3242,  1.1529,  1.3584,  ...,  1.1015,  1.0502,  1.1015]],\n",
       "  \n",
       "           [[-1.9307, -1.9307, -1.9307,  ..., -0.5651, -0.5301, -0.4951],\n",
       "            [-1.9307, -1.9307, -1.9307,  ..., -0.7752, -0.7227, -0.6702],\n",
       "            [-1.9307, -1.9307, -1.9307,  ..., -0.9328, -0.8978, -0.8803],\n",
       "            ...,\n",
       "            [ 1.9734,  2.0084,  2.0784,  ...,  0.9580,  0.9230,  0.8179],\n",
       "            [ 1.7458,  1.7808,  1.9734,  ...,  0.8880,  0.8529,  0.7829],\n",
       "            [ 1.4832,  1.4132,  1.6408,  ...,  0.8179,  0.7479,  0.7829]],\n",
       "  \n",
       "           [[-1.7870, -1.7870, -1.7870,  ..., -1.5081, -1.4907, -1.4733],\n",
       "            [-1.7870, -1.7870, -1.7870,  ..., -1.4559, -1.4384, -1.4036],\n",
       "            [-1.7870, -1.7870, -1.7870,  ..., -1.4210, -1.4384, -1.4384],\n",
       "            ...,\n",
       "            [ 1.9080,  1.9951,  2.0474,  ...,  0.7576,  0.7925,  0.6182],\n",
       "            [ 1.6988,  1.7860,  1.9777,  ...,  0.6705,  0.6879,  0.5834],\n",
       "            [ 1.3677,  1.3502,  1.5942,  ...,  0.7054,  0.6182,  0.5659]]]]),\n",
       "  'label': tensor([1]),\n",
       "  'label_human': ['goldfish', 'Carassius auratus']},\n",
       " 'image_1': {'image': tensor([[[[-1.9467, -1.9295, -1.9467,  ..., -0.8335, -0.8335, -0.7993],\n",
       "            [-1.9809, -1.9638, -1.9467,  ..., -0.8335, -0.8335, -0.8164],\n",
       "            [-1.9809, -1.9809, -1.9638,  ..., -0.8507, -0.8507, -0.8164],\n",
       "            ...,\n",
       "            [-0.5938, -0.5938, -0.5767,  ..., -1.5870, -1.5870, -1.6042],\n",
       "            [-0.5767, -0.5767, -0.5596,  ..., -1.5699, -1.5870, -1.6042],\n",
       "            [-0.5767, -0.5767, -0.5424,  ..., -1.5699, -1.6213, -1.6042]],\n",
       "  \n",
       "           [[-0.9153, -0.9153, -0.9153,  ...,  0.3627,  0.3803,  0.3978],\n",
       "            [-0.9328, -0.9328, -0.9153,  ...,  0.3452,  0.3803,  0.3978],\n",
       "            [-0.9328, -0.9503, -0.9328,  ...,  0.3452,  0.3627,  0.3803],\n",
       "            ...,\n",
       "            [ 0.6604,  0.6604,  0.6779,  ..., -1.0553, -1.0728, -1.0903],\n",
       "            [ 0.6779,  0.6954,  0.6954,  ..., -1.0728, -1.0728, -1.0903],\n",
       "            [ 0.6954,  0.6954,  0.7129,  ..., -1.0728, -1.0903, -1.0903]],\n",
       "  \n",
       "           [[-1.7870, -1.7870, -1.7870,  ..., -0.6367, -0.6541, -0.6367],\n",
       "            [-1.7870, -1.7870, -1.7870,  ..., -0.6367, -0.6541, -0.6367],\n",
       "            [-1.7870, -1.7870, -1.7870,  ..., -0.6541, -0.6541, -0.6367],\n",
       "            ...,\n",
       "            [-0.8633, -0.8284, -0.8110,  ..., -1.7870, -1.7696, -1.7870],\n",
       "            [-0.8458, -0.8110, -0.8110,  ..., -1.7696, -1.7696, -1.7870],\n",
       "            [-0.8284, -0.8110, -0.7936,  ..., -1.7696, -1.7696, -1.7870]]]]),\n",
       "  'label': tensor([94]),\n",
       "  'label_human': ['hummingbird']},\n",
       " 'image_2': {'image': tensor([[[[-0.0116, -0.0458, -0.0801,  ..., -0.4054, -0.3198, -0.2342],\n",
       "            [-0.1314, -0.1314, -0.1828,  ..., -0.4226, -0.3712, -0.2856],\n",
       "            [-0.1657, -0.1657, -0.1828,  ..., -0.4054, -0.3883, -0.3541],\n",
       "            ...,\n",
       "            [-0.4397, -0.4568, -0.1999,  ...,  0.9303,  1.0159,  0.9988],\n",
       "            [-0.0287, -0.0972, -0.2513,  ...,  1.1015,  1.0159,  0.9988],\n",
       "            [ 0.9474,  0.9646,  0.8618,  ...,  0.9988,  0.9474,  0.9303]],\n",
       "  \n",
       "           [[ 0.4853,  0.4678,  0.4853,  ...,  0.1001,  0.1527,  0.1877],\n",
       "            [ 0.3102,  0.3102,  0.3102,  ...,  0.0826,  0.1176,  0.1527],\n",
       "            [ 0.2227,  0.2227,  0.2227,  ...,  0.0651,  0.0826,  0.0826],\n",
       "            ...,\n",
       "            [-0.5826, -0.6352, -0.4601,  ...,  0.6429,  0.6954,  0.6604],\n",
       "            [-0.3375, -0.4426, -0.5301,  ...,  0.8529,  0.7829,  0.7654],\n",
       "            [ 0.4678,  0.4328,  0.3627,  ...,  0.8354,  0.8179,  0.8004]],\n",
       "  \n",
       "           [[ 1.1759,  1.1585,  1.1062,  ...,  0.6705,  0.6879,  0.7054],\n",
       "            [ 0.8099,  0.8274,  0.8274,  ...,  0.6531,  0.7054,  0.7228],\n",
       "            [ 0.4265,  0.4439,  0.5136,  ...,  0.5659,  0.5834,  0.5834],\n",
       "            ...,\n",
       "            [-0.8807, -0.8981, -0.7587,  ...,  0.1128,  0.1825,  0.1302],\n",
       "            [-0.7587, -0.8284, -0.8284,  ...,  0.7054,  0.6008,  0.5659],\n",
       "            [-0.2881, -0.3055, -0.4624,  ...,  0.6879,  0.7054,  0.7228]]]]),\n",
       "  'label': tensor([100]),\n",
       "  'label_human': ['black swan', 'Cygnus atratus']},\n",
       " 'image_3': {'image': tensor([[[[ 1.2214,  1.6153,  1.9920,  ..., -1.7925, -1.7583, -1.7240],\n",
       "            [ 1.2385,  1.8722,  2.0605,  ..., -1.7925, -1.7583, -1.7583],\n",
       "            [ 0.8618,  1.6838,  1.9749,  ..., -1.7925, -1.7925, -1.7754],\n",
       "            ...,\n",
       "            [-1.6042, -1.4843, -1.3987,  ..., -1.3473, -1.2103, -1.2788],\n",
       "            [-1.5699, -1.6042, -1.6213,  ..., -1.2788, -1.0390, -1.1760],\n",
       "            [-1.4329, -1.6384, -1.5870,  ..., -1.1932, -1.0048, -1.1932]],\n",
       "  \n",
       "           [[ 0.8880,  1.4132,  1.9209,  ..., -1.8606, -1.8256, -1.7906],\n",
       "            [ 0.9755,  1.8158,  2.1310,  ..., -1.8606, -1.8256, -1.8081],\n",
       "            [ 0.6254,  1.6232,  2.0609,  ..., -1.8782, -1.8606, -1.8431],\n",
       "            ...,\n",
       "            [-1.1954, -1.0553, -0.9678,  ..., -0.7752, -0.6176, -0.7052],\n",
       "            [-1.1604, -1.2129, -1.2304,  ..., -0.6702, -0.4251, -0.6176],\n",
       "            [-1.0203, -1.2304, -1.1954,  ..., -0.5826, -0.3725, -0.6001]],\n",
       "  \n",
       "           [[-1.2816, -1.4036, -1.4733,  ..., -1.7173, -1.7173, -1.7173],\n",
       "            [-1.5256, -1.6302, -1.7696,  ..., -1.7173, -1.7347, -1.7522],\n",
       "            [-1.7696, -1.6650, -1.7696,  ..., -1.7347, -1.7522, -1.7696],\n",
       "            ...,\n",
       "            [-1.7696, -1.7173, -1.6824,  ..., -1.3339, -1.2467, -1.3687],\n",
       "            [-1.7522, -1.7696, -1.7870,  ..., -1.3513, -1.1770, -1.3687],\n",
       "            [-1.7347, -1.8044, -1.7347,  ..., -1.3687, -1.2293, -1.4384]]]]),\n",
       "  'label': tensor([207]),\n",
       "  'label_human': ['golden retriever']},\n",
       " 'image_4': {'image': tensor([[[[-1.0219, -1.0219, -1.0219,  ..., -1.0733, -1.0904, -1.1418],\n",
       "            [-1.0733, -1.0390, -1.0562,  ..., -1.0733, -1.1075, -1.1075],\n",
       "            [-1.0904, -1.0904, -1.0904,  ..., -1.0904, -1.0904, -1.0733],\n",
       "            ...,\n",
       "            [ 0.2624,  0.1254, -0.0629,  ..., -1.6042, -1.5870, -1.4843],\n",
       "            [ 0.3481,  0.3994,  0.2282,  ..., -1.5870, -1.5870, -1.5528],\n",
       "            [ 0.4508,  0.2796, -0.0116,  ..., -1.6042, -1.6042, -1.6042]],\n",
       "  \n",
       "           [[ 0.0826,  0.1176,  0.1527,  ..., -0.1275, -0.1450, -0.1800],\n",
       "            [ 0.0651,  0.1001,  0.1176,  ..., -0.1275, -0.1625, -0.1625],\n",
       "            [ 0.0301,  0.0651,  0.0651,  ..., -0.1275, -0.1450, -0.1275],\n",
       "            ...,\n",
       "            [ 1.2731,  1.1331,  0.9755,  ..., -1.2479, -1.2129, -1.1078],\n",
       "            [ 1.1155,  1.1681,  1.0980,  ..., -1.2304, -1.2304, -1.1954],\n",
       "            [ 1.1681,  1.1155,  0.9230,  ..., -1.2479, -1.2829, -1.2829]],\n",
       "  \n",
       "           [[-0.9156, -0.9156, -0.9156,  ..., -0.8110, -0.8110, -0.8458],\n",
       "            [-0.9156, -0.9156, -0.9156,  ..., -0.8110, -0.8110, -0.8284],\n",
       "            [-0.9330, -0.9330, -0.9330,  ..., -0.8284, -0.8284, -0.8633],\n",
       "            ...,\n",
       "            [-0.1312, -0.2010, -0.3578,  ..., -0.9330, -0.9504, -0.9330],\n",
       "            [-0.0964, -0.0790, -0.3055,  ..., -0.9330, -0.9678, -0.9678],\n",
       "            [-0.1138, -0.3055, -0.5495,  ..., -0.9504, -0.9330, -0.9330]]]]),\n",
       "  'label': tensor([985]),\n",
       "  'label_human': ['daisy']}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading and transforming the images\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "img1 = transform(Image.open(\"datasets/my_dataset/1.jpg\")).unsqueeze(0)\n",
    "img2 = transform(Image.open(\"datasets/my_dataset/2.jpg\")).unsqueeze(0)\n",
    "img3 = transform(Image.open(\"datasets/my_dataset/3.jpg\")).unsqueeze(0)\n",
    "img4 = transform(Image.open(\"datasets/my_dataset/4.jpg\")).unsqueeze(0)\n",
    "img5 = transform(Image.open(\"datasets/my_dataset/5.jpg\")).unsqueeze(0)\n",
    "image_data = {\n",
    "    'image_0': {\n",
    "        'image': img1,\n",
    "        'label': torch.tensor([1]),  # Replace with actual label index\n",
    "        'label_human': ['goldfish','Carassius auratus'],  # Replace with actual human-readable label\n",
    "    },\n",
    "\n",
    "    'image_1': {\n",
    "        'image': img2,\n",
    "        'label': torch.tensor([94]),  # Replace with actual label index\n",
    "        'label_human': ['hummingbird'],  # Replace with actual human-readable label\n",
    "    },\n",
    "\n",
    "    'image_2': {\n",
    "        'image': img3,\n",
    "        'label': torch.tensor([100]),  # Replace with actual label index\n",
    "        'label_human': ['black swan', 'Cygnus atratus'],  # Replace with actual human-readable label\n",
    "    },\n",
    "\n",
    "    'image_3': {\n",
    "        'image': img4,\n",
    "        'label': torch.tensor([207]),  # Replace with actual label index\n",
    "        'label_human': ['golden retriever'],  # Replace with actual human-readable label\n",
    "    },\n",
    "\n",
    "    'image_4': {\n",
    "        'image': img5,\n",
    "        'label': torch.tensor([985]),  # Replace with actual label index\n",
    "        'label_human': ['daisy'],  # Replace with actual human-readable label\n",
    "    }\n",
    "\n",
    "}\n",
    "\n",
    "image_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explainer(model, labels_human, DEVICE):\n",
    "    model.to(DEVICE)\n",
    "    model.eval()\n",
    "\n",
    "    # Explainer \n",
    "    attribution = Saliency(model)\n",
    "\n",
    "    # # Load images and labels\n",
    "    # with open(\"sample_imagenetdata\", 'rb') as f:\n",
    "    #     image_data = pickle.load(f)\n",
    "    fig, ax = plt.subplots(5,3, figsize=(30,50))\n",
    "    i=0\n",
    "    for key,value in image_data.items():\n",
    "        X = value['image']\n",
    "        y = value['label']\n",
    "        label = value['label_human']\n",
    "        X, y = X.to(DEVICE), y.to(DEVICE)\n",
    "        # Predict the label\n",
    "        output = model(X)\n",
    "        probabilities = torch.nn.functional.softmax(output[0], dim=0)\n",
    "        predicted_label = torch.max(probabilities, 0)[1].item()\n",
    "\n",
    "        # Compute the attribution scores using Saliency for true label\n",
    "        attr_true = attribution.attribute(inputs=X, target=y)\n",
    "\n",
    "        # Compute the attribution scores using Saliency for predicted label\n",
    "        attr_pred = attribution.attribute(inputs=X, target=predicted_label)\n",
    "\n",
    "        # Transform the image to original scale\n",
    "        X = X * torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1) + torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1)\n",
    "        \n",
    "        # Visualize the attribution scores for true label\n",
    "        explainer_true, _ = torch.max(attr_true.data.abs(), dim=1) \n",
    "        explainer_true = explainer_true.cpu().detach().numpy()\n",
    "        explainer_true = (explainer_true-explainer_true.min())/(explainer_true.max()-explainer_true.min())\n",
    "\n",
    "        # Visualize the attribution scores for predicted label\n",
    "        explainer_pred, _ = torch.max(attr_pred.data.abs(), dim=1)\n",
    "        explainer_pred = explainer_pred.cpu().detach().numpy()\n",
    "        explainer_pred = (explainer_pred-explainer_pred.min())/(explainer_pred.max()-explainer_pred.min())\n",
    "        ax[i][0].imshow(X[0].permute(1, 2, 0).to('cpu'))\n",
    "        ax[i][1].imshow(explainer_true[0])\n",
    "        ax[i][1].set_title(f\"True: {label[0]}\", fontsize=48)\n",
    "        ax[i][2].imshow(explainer_pred[0])\n",
    "        ax[i][2].set_title(f\"Predicted: {labels_human[predicted_label][0]}\", fontsize=48)\n",
    "        ax[i][0].set_xticks([])\n",
    "        ax[i][0].set_yticks([])\n",
    "        ax[i][1].set_xticks([])\n",
    "        ax[i][1].set_yticks([])\n",
    "        ax[i][2].set_xticks([])\n",
    "        ax[i][2].set_yticks([])\n",
    "        i+=1\n",
    "    fig.subplots_adjust(wspace=0, hspace=0, top=1.0)\n",
    "    plt.savefig(\"Saliency2.png\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=GoogLeNet_Weights.IMAGENET1K_V1`. You can also use `weights=GoogLeNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\captum\\_utils\\gradient.py:57: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "\n",
    "    # Create the model\n",
    "    model_googlenet= models.googlenet(pretrained=True)\n",
    "\n",
    "    # Summary of the model\n",
    "    # print(summary(model=model_googlenet, input_size=(1, 3, 224, 224), col_width=20, col_names=['input_size', 'output_size', 'num_params', 'trainable'], row_settings=['var_names'], verbose=0))\n",
    "    \n",
    "    # Load classes to human readable labels\n",
    "    labels_human = {}\n",
    "    with open(f'imagenet1000_clsidx_to_labels.txt') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            line = line.strip().replace(\"'\", \"\").strip(\",\")\n",
    "            if \"{\" in line or \"}\" in line:\n",
    "                continue\n",
    "            else:\n",
    "                idx = int(line.split(\":\")[0])\n",
    "                lbl = line.split(\":\")[1].split(\",\")\n",
    "                labels_human[idx] = [x.strip() for x in lbl]\n",
    "    # Explainer\n",
    "    explainer(model_googlenet, labels_human,  DEVICE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
